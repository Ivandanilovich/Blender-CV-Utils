{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286a6b8c-eff5-425f-a969-b15f03f5c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/ivand/Documents/Projects/Pigs/Blender-CV-Utils')\n",
    "\n",
    "from blender_cv_utils.trackable import SceneObject\n",
    "import time\n",
    "import bpy\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084c7bfe-b7d0-4099-a572-d9c2dcd37e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial Render class input\n",
    "output_path = 'C:/Users/ivand/Desktop/ses/'\n",
    "output_suffix='default' \n",
    "blender_file_path= 'C:/Users/ivand/Downloads/test scene.blend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eebd1bc-4f9b-4f71-a1b9-aa2dd77376ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blender_file_path:\n",
    "    bpy.ops.wm.open_mainfile(filepath=blender_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81831632-5942-4648-a771-7c981d0cb349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene = bpy.context.scene\n",
    "scene.frame_end\n",
    "scene.frame_start\n",
    "scene.frame_current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32a6d40-2943-4db7-972d-59363cdcb77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneObject():\n",
    "    def __init__(self, ob):\n",
    "        if isinstance(ob, str):\n",
    "            assert ob in bpy.data.objects.keys(), 'Object with that name is not at scene'\n",
    "            self._name = ob\n",
    "        elif not isinstance(ob, bpy_types.Object):\n",
    "            raise Exception('Not blender object')\n",
    "        else:\n",
    "            self._name = ob.name\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "    \n",
    "    @property\n",
    "    def ob(self):\n",
    "        return bpy.data.objects[self.name] # safier to call object by name each time\n",
    "    \n",
    "    @property\n",
    "    def type(self):\n",
    "        return self.ob.type\n",
    "    \n",
    "    def get_annotations(self, depsgraph):\n",
    "        ob = depsgraph.objects[self._name]\n",
    "        return {\n",
    "            'type': self.type,\n",
    "            'name': self.name,\n",
    "            'location': tuple(ob.location),\n",
    "            'scale': tuple(ob.scale),\n",
    "            'rotation_euler': tuple(ob.rotation_euler),\n",
    "        }\n",
    "\n",
    "class MeshObject(SceneObject):\n",
    "    def __init__(self, ob, is_mask=True, is_xraymask=False):\n",
    "        super().__init__(ob)\n",
    "        assert self.type == 'MESH'\n",
    "        self.is_mask = is_mask\n",
    "        self.is_xraymask = is_xraymask\n",
    "\n",
    "    def get_annotations(self, depsgraph, **kwargs):\n",
    "        super_annots = super().get_annotations(depsgraph)\n",
    "        ob = depsgraph.objects[self._name]\n",
    "        d = {\n",
    "            'verts': [tuple(i.co) for i in ob.data.vertices], \n",
    "            'ray_cast_collision_points': {},\n",
    "        }\n",
    "        if 'cameras' in kwargs:\n",
    "            for camera in kwargs['cameras']:\n",
    "                camera_location = depsgraph.objects[camera.name].location\n",
    "                ray_cast_collision_points = [tuple(scene.ray_cast(depsgraph, camera_location, i.co-camera_location)[1]) for i in ob.data.vertices] \n",
    "                d['ray_cast_collision_points'][camera.name] = ray_cast_collision_points\n",
    "        return super_annots | d\n",
    "\n",
    "\n",
    "class ArmatureObject(SceneObject):\n",
    "    def __init__(self, ob):\n",
    "        super().__init__(ob)\n",
    "        assert self.type == 'ARMATURE'\n",
    "\n",
    "    def get_annotations(self, depsgraph, **kwargs):\n",
    "        super_annots = super().get_annotations(depsgraph)\n",
    "        ob = depsgraph.objects[self._name]\n",
    "\n",
    "        default_data={}\n",
    "        for bone in ob.data.bones:\n",
    "            default_data[bone.name] = {'head_local': tuple(bone.head_local),\n",
    "                                        'head': tuple(bone.head),\n",
    "                                        'tail_local': tuple(bone.tail_local),\n",
    "                                        'tail': tuple(bone.tail),\n",
    "                                        'matrix': tuple([tuple(i) for i in bone.matrix]),\n",
    "                                        'matrix_local': tuple([tuple(i) for i in bone.matrix_local]),}\n",
    "        pose={}\n",
    "        for bone in ob.pose.bones:\n",
    "            pose[bone.name] = {'head':tuple(bone.head),\n",
    "                               'tail':tuple(bone.tail),\n",
    "                               'matrix':tuple([tuple(i) for i in bone.matrix]),}\n",
    "        d = {\n",
    "            'default_data': default_data,\n",
    "            'pose': pose,\n",
    "        }\n",
    "        return super_annots | d\n",
    "    \n",
    "\n",
    "\n",
    "class CameraObject(SceneObject):\n",
    "    def __init__(self, ob):\n",
    "        super().__init__(ob)\n",
    "        assert self.type == 'CAMERA'\n",
    "\n",
    "    def get_annotations(self, depsgraph, **kwargs):\n",
    "        super_annots = super().get_annotations(depsgraph)\n",
    "        ob = depsgraph.objects[self._name]\n",
    "        d = {\n",
    "            'lens': ob.data.lens,\n",
    "            'lens_unit': ob.data.lens_unit,\n",
    "            'angle_x': ob.data.angle_x,\n",
    "            'angle_y': ob.data.angle_y,\n",
    "            'sensor_fit': ob.data.sensor_fit,\n",
    "            'sensor_height': ob.data.sensor_height,\n",
    "            'sensor_width': ob.data.sensor_width,\n",
    "            # 'matrix_world': tuple([tuple(i) for i in ob.data.matrix_world]),\n",
    "        }\n",
    "        return super_annots | d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a64a9c-a1c6-4ae1-99e8-6e116adfb849",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameras = [\n",
    "    CameraObject('Camera'),\n",
    "    # CameraObject('Camera.001'),\n",
    "]\n",
    "\n",
    "obs = [\n",
    "    MeshObject('Cube', is_mask=True, is_xraymask=False),\n",
    "    # MeshObject('Icosphere', is_mask=True, is_xraymask=False),\n",
    "    # ArmatureObject('Armature'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc4e8a08-e603-4056-a05d-fe13956e9ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bpy.data.scenes['Scene'].node_tree"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initiate cryptomatte\n",
    "if not scene.use_nodes:\n",
    "    scene.use_nodes = True\n",
    "scene.render.use_compositing = True\n",
    "scene.view_layers[\"ViewLayer\"].use_pass_cryptomatte_object = True\n",
    "\n",
    "output_nodes=[]\n",
    "for ob in obs:\n",
    "    if ob.type!='MESH' or not ob.is_mask:\n",
    "        continue\n",
    "    output_node = scene.node_tree.nodes.new(\"CompositorNodeOutputFile\")\n",
    "    output_node.base_path = os.path.join(output_path, output_suffix, ob.name)\n",
    "    output_nodes.append(output_node)\n",
    "    cryptomatte = scene.node_tree.nodes.new(\"CompositorNodeCryptomatteV2\")\n",
    "    cryptomatte.matte_id = ob.name\n",
    "    scene.node_tree.links.new(cryptomatte.outputs['Image'], output_node.inputs['Image'])\n",
    "    scene.node_tree.links.new(cryptomatte.inputs['Image'], scene.node_tree.nodes['Render Layers'].outputs['Image'])\n",
    "\n",
    "\n",
    "output_node = scene.node_tree.nodes.new(\"CompositorNodeOutputFile\")\n",
    "output_node.base_path = os.path.join(output_path, output_suffix)\n",
    "scene.node_tree.links.new(scene.node_tree.nodes['Render Layers'].outputs['Image'], output_node.inputs['Image'])       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61cf876-0c8c-40a3-aae8-71e894e2a238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635f91ac9bfd497faf9a866cc1a45c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(scene.frame_start, scene.frame_end)):\n",
    "    scene.frame_current = i\n",
    "    depsgraph = bpy.context.evaluated_depsgraph_get()\n",
    "    annots = {ob.name:ob.get_annotations(depsgraph, cameras=cameras) for ob in obs+cameras}\n",
    "    for camera in cameras:\n",
    "        scene.camera = camera.ob\n",
    "        for node in scene.node_tree.nodes:\n",
    "            if node.type == 'OUTPUT_FILE':\n",
    "                node.file_slots[0].path = f'{camera.name}_'\n",
    "        bpy.ops.render.render()\n",
    "    json.dump(annots, open(os.path.join(output_path, output_suffix, f'{i:04}.json'),'w'), indent=4)\n",
    "    break\n",
    "\n",
    "\n",
    "json.dump({'resolution_percentage': scene.render.resolution_percentage,\n",
    "                        'resolution_x': scene.render.resolution_x,\n",
    "                        'resolution_y': scene.render.resolution_y,\n",
    "                        'pixel_aspect_x': scene.render.pixel_aspect_x,\n",
    "                        'pixel_aspect_y': scene.render.pixel_aspect_y,\n",
    "                        'engine': scene.render.engine,}, \n",
    "          open(os.path.join(output_path, output_suffix, f'global_params.json'),'w'), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6506ea75-1065-43e1-affe-0c21c622f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_params = {'resolution_percentage': scene.render.resolution_percentage,\n",
    "                        'resolution_x': scene.render.resolution_x,\n",
    "                        'resolution_y': scene.render.resolution_y,\n",
    "                        'pixel_aspect_x': scene.render.pixel_aspect_x,\n",
    "                        'pixel_aspect_y': scene.render.pixel_aspect_y,\n",
    "                        'engine': scene.render.engine,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b00d885b-eff2-43b4-9951-3f3873492bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'CAMERA',\n",
       " 'name': 'Camera',\n",
       " 'location': (7.358891487121582, -6.925790786743164, 4.958309173583984),\n",
       " 'scale': (1.0, 1.0, 1.0),\n",
       " 'rotation_euler': (1.1093189716339111, 0.0, 0.8149281740188599),\n",
       " 'lens': 50.0,\n",
       " 'lens_unit': 'MILLIMETERS',\n",
       " 'angle_x': 0.6911112070083618,\n",
       " 'angle_y': 0.4710899591445923,\n",
       " 'sensor_fit': 'AUTO',\n",
       " 'sensor_height': 24.0,\n",
       " 'sensor_width': 36.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annots['Camera']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0edaccb1-0bdb-4763-8125-1a921be09f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rotation_euler\n",
      "[1.10931897 0.         0.81492817]\n",
      "[[ 0.68592066  0.72767633  0.        ]\n",
      " [-0.32401347  0.30542086  0.89539565]\n",
      " [ 0.65155822 -0.61417038  0.44527141]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "class ProjectionCamera():\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def get_sensor_size(self, sensor_fit, sensor_x, sensor_y):\n",
    "        if sensor_fit == 'VERTICAL':\n",
    "            return sensor_y\n",
    "        return sensor_x\n",
    "\n",
    "    # BKE_camera_sensor_fit\n",
    "    def get_sensor_fit(self, sensor_fit, size_x, size_y):\n",
    "        if sensor_fit == 'AUTO':\n",
    "            if size_x >= size_y:\n",
    "                return 'HORIZONTAL'\n",
    "            else:\n",
    "                return 'VERTICAL'\n",
    "        return sensor_fit\n",
    "\n",
    "    # Build intrinsic camera parameters from Blender camera data\n",
    "    #\n",
    "    # See notes on this in \n",
    "    # blender.stackexchange.com/questions/15102/what-is-blenders-camera-projection-matrix-model\n",
    "    # as well as\n",
    "    # https://blender.stackexchange.com/a/120063/3581\n",
    "    def get_calibration_matrix_K(self, resolution_x, resolution_y, sensor_width, sensor_height, lens, sensor_fit,\n",
    "                                 shift_x=0, shift_y=0, \n",
    "                                 pixel_aspect_x=1, pixel_aspect_y=1, resolution_percentage=100):\n",
    "        resolution_x_in_px = resolution_x * resolution_percentage/100\n",
    "        resolution_y_in_px = resolution_y * resolution_percentage/100\n",
    "        sensor_size_in_mm = self.get_sensor_size(sensor_fit, sensor_width, sensor_height)\n",
    "        sensor_fit = self.get_sensor_fit(sensor_fit, pixel_aspect_x * resolution_x_in_px, pixel_aspect_y * resolution_y_in_px)\n",
    "        pixel_aspect_ratio = pixel_aspect_y / pixel_aspect_x\n",
    "        if sensor_fit == 'HORIZONTAL':\n",
    "            view_fac_in_px = resolution_x_in_px\n",
    "        else:\n",
    "            view_fac_in_px = pixel_aspect_ratio * resolution_y_in_px\n",
    "        pixel_size_mm_per_px = sensor_size_in_mm / lens / view_fac_in_px\n",
    "        s_u = 1 / pixel_size_mm_per_px\n",
    "        s_v = 1 / pixel_size_mm_per_px / pixel_aspect_ratio\n",
    "\n",
    "        # Parameters of intrinsic calibration matrix K\n",
    "        u_0 = resolution_x_in_px / 2 - shift_x * view_fac_in_px\n",
    "        v_0 = resolution_y_in_px / 2 + shift_y * view_fac_in_px / pixel_aspect_ratio\n",
    "        skew = 0 # only use rectangular pixels\n",
    "\n",
    "        K = np.array([(s_u, skew, u_0),\n",
    "            (   0,  s_v, v_0),\n",
    "            (   0,    0,   1)])\n",
    "        return K\n",
    "\n",
    "    def get_RT_matrix(self, rotation_euler, location):\n",
    "        # bcam stands for blender camera\n",
    "        R_bcam2cv = np.array(((1, 0,  0),\n",
    "            (0, -1, 0),\n",
    "            (0, 0, -1)))\n",
    "    \n",
    "        # Transpose since the rotation is object rotation, \n",
    "        # and we want coordinate rotation\n",
    "        R_world2bcam = Rotation.from_euler('xyz', rotation_euler).as_matrix().T\n",
    "        T_world2bcam = -1*R_world2bcam @ location\n",
    "        \n",
    "        # # Use matrix_world instead to account for all constraints\n",
    "        # location, rotation = cam.matrix_world.decompose()[0:2]\n",
    "        # R_world2bcam = rotation.to_matrix().transposed()\n",
    "    \n",
    "        # Convert camera location to translation vector used in coordinate changes\n",
    "        # T_world2bcam = -1*R_world2bcam @ cam.location\n",
    "        # Use location from matrix_world to account for constraints:     \n",
    "        # T_world2bcam = -1*R_world2bcam @ location\n",
    "    \n",
    "        # Build the coordinate transform matrix from world to computer vision camera\n",
    "        R_world2cv = R_bcam2cv@R_world2bcam\n",
    "        T_world2cv = R_bcam2cv@T_world2bcam\n",
    "    \n",
    "        # put into 3x4 matrix\n",
    "        RT = np.concatenate((R_world2cv, T_world2cv[:,None]), axis=1)\n",
    "        return RT\n",
    "\n",
    "K = ProjectionCamera().get_calibration_matrix_K(global_params['resolution_x'], \n",
    "                                            global_params['resolution_y'],\n",
    "                                            annots['Camera']['sensor_width'],\n",
    "                                            annots['Camera']['sensor_height'],\n",
    "                                            annots['Camera']['lens'],\n",
    "                                            annots['Camera']['sensor_fit'])\n",
    "\n",
    "RT = ProjectionCamera().get_RT_matrix(np.array(annots['Camera']['rotation_euler']), \n",
    "                                 np.array(annots['Camera']['location']))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69ef040d-13e6-4065-bf6c-6e0791dac437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.3589, -6.9258,  4.9583])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(annots['Camera']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b94b9ec2-7b01-4924-8e27-3f15e7e442e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.85920661e-01,  7.27676334e-01,  0.00000000e+00,\n",
       "        -7.88166858e-03],\n",
       "       [ 3.24013467e-01, -3.05420860e-01, -8.95395651e-01,\n",
       "        -6.00124509e-02],\n",
       "       [-6.51558224e-01,  6.14170377e-01, -4.45271410e-01,\n",
       "         1.12561551e+01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1a885cf-d975-4ace-825e-81c31531d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = (K @ RT) @ np.array((0,0,0,1))\n",
    "pc /= pc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7049b932-f04c-4827-ab1a-38e3577f6561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([958.13277423, 525.78260511,   1.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "042a6e8e-ea69-4a3d-b2b5-b68007510462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.68592066, -0.32401347,  0.65155822, 10.52229232],\n",
       "       [-0.72767633, -0.30542086,  0.61417038, -0.19436358],\n",
       "       [ 0.        , -0.89539565, -0.44527141,  3.99352963]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in module unregister(): 'C:\\\\Users\\\\ivand\\\\AppData\\\\Roaming\\\\Blender Foundation\\\\Blender\\\\3.6\\\\scripts\\\\addons\\\\bpy-regex-selector\\\\__init__.py'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ivand\\Downloads\\blender-3.6.5-windows-x64\\3.6\\scripts\\modules\\addon_utils.py\", line 421, in disable\n",
      "    mod.unregister()\n",
      "  File \"C:\\Users\\ivand\\AppData\\Roaming\\Blender Foundation\\Blender\\3.6\\scripts\\addons\\bpy-regex-selector\\__init__.py\", line 64, in unregister\n",
      "    bpy.utils.unregister_class(cls)\n",
      "RuntimeError: unregister_class(...):, missing bl_rna attribute from 'RNAMeta' instance (may not be registered)\n"
     ]
    }
   ],
   "source": [
    "RT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17e770e2-87ea-4ae5-995b-fe6a4d13b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = plt.imread('C:/Users/ivand/Downloads/t.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8ebd0-2a19-4e26-8ec0-89858ed1cc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e04583-2c8a-471e-bcc2-55afe6ee24d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ffe76-cb92-406e-b1a1-555cf12db6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = [SceneObject('Cube')]\n",
    "\n",
    "\n",
    "class Renderer():\n",
    "    def __init__(self, cameras, obs, output_path, output_suffix='default', blender_file_path=None):\n",
    "        bpy.ops.wm.open_mainfile(filepath=blender_file_path)\n",
    "        time.sleep(1)\n",
    "        assert len(list(bpy.data.scenes)) == 1\n",
    "\n",
    "        self.cameras = cameras\n",
    "        self.obs = obs\n",
    "        self.frame_index = 0\n",
    "\n",
    "        time_suffix = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "        self.output_path = os.path.join(output_path, f'{output_suffix}_{time_suffix}')\n",
    "        os.mkdir(self.output_path)\n",
    "        \n",
    "        self.\n",
    "        self.resolution_percentage = self.scene.render.resolution_percentage\n",
    "        self.resolution_x = self.scene.render.resolution_x\n",
    "        self.resolution_y = self.scene.render.resolution_y\n",
    "        self.pixel_aspect_x = self.scene.render.pixel_aspect_x\n",
    "        self.pixel_aspect_y = self.scene.render.pixel_aspect_y\n",
    "        self.engine = self.scene.render.engine\n",
    "\n",
    "        # also save global params that is not change during generation\n",
    "\n",
    "        pickle.dump({'resolution_percentage': self.resolution_percentage,\n",
    "                        'resolution_x': self.resolution_x,\n",
    "                        'resolution_y': self.resolution_y,\n",
    "                        'pixel_aspect_x': self.pixel_aspect_x,\n",
    "                        'pixel_aspect_y': self.pixel_aspect_y,\n",
    "                        'engine': self.engine,}, \n",
    "                    open(os.path.join(self.output_path, 'render_params.pickle'), 'wb'))\n",
    "\n",
    "        self._init_cryptomatte()\n",
    "\n",
    "    def _init_cryptomatte(self):\n",
    "        if not self.scene.use_nodes:\n",
    "            self.scene.use_nodes = True\n",
    "        else:\n",
    "            print('Nodes initiated')\n",
    "            return\n",
    "        self.\n",
    "\n",
    "    def get_mask_for_object(self, ob):\n",
    "        # bpy.ops.render.render(write_still=True)\n",
    "        # self.scene.render.use_compositing = True # update nodes\n",
    "        self.scene.node_tree.nodes['Cryptomatte'].matte_id = ob.name\n",
    "        # self.scene.render.use_compositing = True # update nodes\n",
    "        time.sleep(1)  # blender tackle\n",
    "        pixels = bpy.data.images['Viewer Node'].pixels\n",
    "        image = np.array(pixels).reshape(self.resolution_y,self.resolution_x,4)\n",
    "        return image[::-1] # don't know why but this image is turned upside down\n",
    "    \n",
    "    def render(self, image_path, render_engine=None):\n",
    "        if render_engine:\n",
    "            self.scene.render.engine = render_engine\n",
    "\n",
    "        self.scene.render.filepath = image_path\n",
    "        bpy.ops.render.render(write_still=True)\n",
    "        self.scene.render.engine = self.engine\n",
    "        \n",
    "    \n",
    "    def get_xray_mask_for_object(self, path, ob):\n",
    "        for o in self.obs:\n",
    "            if o.type == \"MESH\":\n",
    "                o.hide_render = True\n",
    "\n",
    "        ob.hide_render = False\n",
    "        self.render(path, render_engine='BLENDER_WORKBENCH')\n",
    "\n",
    "        for o in self.obs:\n",
    "            if o.type == \"MESH\":\n",
    "                o.hide_render = False\n",
    "\n",
    "    def main(self):\n",
    "        annots = {ob.name:ob.get_annotations() for ob in self.obs}\n",
    "        cameras = [ob for ob in self.obs if ob.type=='CAMERA']\n",
    "        meshes = [ob for ob in self.obs if ob.type=='MESH']\n",
    "        for camera in cameras:\n",
    "            camera.make_active()\n",
    "            impath = os.path.join(self.output_root_path, self.output_suffix, f'image_{camera.name}.png')\n",
    "            self.render(image_path=impath)\n",
    "            annots[camera.name]['image_path'] = impath\n",
    "\n",
    "            for mesh in meshes: # получаем маски для объектов\n",
    "                mask_path = os.path.join(self.output_root_path, self.output_suffix, f'image_{camera.name}_mask_{mesh.name}.png')\n",
    "                mask = self.get_mask_for_object(mesh)\n",
    "                plt.imsave(mask_path, mask)\n",
    "                annots[mesh.name]['mask_path'] = mask_path\n",
    "\n",
    "                # xray маска для объекта\n",
    "                mask_path = os.path.join(self.output_root_path, self.output_suffix, f'image_{camera.name}_xraymask_{mesh.name}.png')\n",
    "                self.get_xray_mask_for_object(mask_path, mesh)\n",
    "                annots[mesh.name]['xraymask_path'] = mask_path\n",
    "\n",
    "\n",
    "        pickle.dump(annots, open(os.path.join(self.output_root_path, self.output_suffix, f'annots_{self.frame_index}.pickle'), 'wb'))\n",
    "        self.frame_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43734ba0-24e6-4067-886b-8c81c25bb33b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c3adc2-5b57-446f-9353-b9258b198ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6974ad-f209-48fe-a282-ae9a90d6eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08536a21-78c3-4a84-8416-2c1a8ef3ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = bpy.data.objects['Camera'].location\n",
    "v = np.array((0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44d700a2-ba5e-40ed-b8de-2feece7e0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " Vector((1.0, -0.9411458969116211, 0.6737847328186035)),\n",
       " Vector((1.0, -0.0, 0.0)),\n",
       " 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpy.data.objects['Cube'].ray_cast(l,v-l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ddaa677c-d16e-4b57-a84f-4e35b8110938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False <Vector (1.5299, 0.5302, 1.6099)> [1.         0.54129553 1.30652177]\n",
      "False <Vector (1.2786, 1.2903, -0.3566)> [ 1.          1.30652177 -0.54129553]\n",
      "True <Vector (1.0000, -1.3065, 0.5413)> [ 1.         -1.30652177  0.54129553]\n",
      "True <Vector (1.0000, -0.5413, -1.3065)> [ 1.         -0.54129553 -1.30652177]\n",
      "False <Vector (1.3495, 1.1818, 1.8467)> [-1.          1.30652177  0.54129553]\n",
      "False <Vector (1.5042, 0.4937, 0.2909)> [-1.          0.54129553 -1.30652177]\n",
      "True <Vector (-1.0000, -0.5413, 1.3065)> [-1.         -0.54129553  1.30652177]\n",
      "False <Vector (0.9094, -1.1857, 0.6116)> [-1.         -1.30652177 -0.54129553]\n"
     ]
    }
   ],
   "source": [
    "for i in depsgraph.objects['Cube'].data.vertices:\n",
    "    i = np.array(i.co)\n",
    "    \n",
    "    print(np.sqrt((r[0]-i[0])**2+(r[1]-i[1])**2+(r[2]-i[2])**2)<1e-5, r, i, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "caf9043a-a0d0-42f2-9ee6-a7f1a1fdcf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.233796944309421"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65602a8c-80fa-489c-b5b0-188f3ae0e9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vector((21.452486038208008, 0.11471939086914062, 13.015824317932129))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depsgraph.objects['Camera'].location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb4129-84c4-41f0-ba80-3d60ee8b0364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blender",
   "language": "python",
   "name": "blender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
